import pandas as pd
import numpy as np

'''Load the dataset with low_memory=False, this helps pandas read large CSV files 
more efficiently, and can resolve issues like mixed data types.'''

players = pd.read_csv("players_22.csv", low_memory=False)

# Features to cluster on 
features = ["overall", "potential", "wage_eur", "value_eur", "age"]

# Drops any rows where any columns are null or missing value 
players = players.dropna(subset=features)

# Copy the relevant features for clustering
data = players[features].copy()

# K-means Clustering Steps 
'''1. Scale the data (make sure all values are in the range 1-10, so no one value dominates the clustering)
   2. Initialize random centroids
   3. Label each data point (based on the distance from centroid)
   4. Update Centroids 
   5. Repeat steps 3 and 4 until centroids stop changing
'''

# Scaling data (MinMax Scale)
data = ((data - data.min()) / (data.max() - data.min())) * 9 + 1

# Display basic statistics of the scaled data
print("Data Description:\n", data.describe())

# Display the first few rows of the scaled data
print("\nFirst 5 Rows of Data:\n", data.head())

# Initializing Random Centroids 
def random_centroids(data, k):
    centroids = []
    for i in range(k):
        centroid = data.apply(lambda x: float(x.sample().iloc[0]))
        '''This randomly samples a single value from each column, turns it into a float,
        iloc[0] is simply used to extract the first element from the sampled Series'''
        centroids.append(centroid)
    return pd.concat(centroids, axis=1)

centroids = random_centroids(data, 5)

print(centroids)

# Labelling each datapoint according to centroid distance
def get_labels(data, centroids):
    # Calculating distance
    distances = centroids.apply(lambda x: np.sqrt(((data - x) ** 2).sum(axis=1)))
    return distances.idxmin(axis=1)

labels = get_labels(data, centroids)
print(labels)

# How many players in each cluster (each unique value occurs in cluster)
print(labels.value_counts())

# Updating Centroids based on which players in cluster
# Takes natural log of each data point, then finds mean of all values in row, then does e^(mean value)
# This gives the geometric mean
def new_centroids(data, labels, k):
    return data.groupby(labels).apply(lambda x: np.exp(np.log(x).mean())).T
#.T switches rows and columns 

# Visualization 
# PCA stands for principle components analysis (will turn the 5 dimensional data currently have into 2 dimensional)
from sklearn.decomposition import PCA
import matplotlib.pyplot as plt 

def plot_clusters(data, labels, centroids, iteration):
    pca = PCA(n_components=2)
    data_2d = pca.fit_transform(data)
    centroids_2d = pca.transform(centroids.T)
    plt.title(f'Iteration {iteration}')
    plt.scatter(x=data_2d[:,0], y=data_2d[:,1], c=labels)
    plt.scatter(x=centroids_2d[:,0], y=centroids_2d[:,1])
    plt.show()

max_iterations =100 #number of times algorithm will iterate 
k = 5 #number of clusters 
centroids = random_centroids(data,k)
old_centroids = pd.DataFrame()
iteration = 1

while iteration < max_iterations and not centroids.equals(old_centroids):
    old_centroids = centroids 
    labels = get_labels (data, centroids)
    centroids = new_centroids(data, labels,k)
    plot_clusters(data, labels, centroids, iteration)
    iteration +=1

#What each represent
print(centroids)

#Which players its referring to (star players)
players[labels==0][["short_name"]+ features]
#Label 1 is younger players
#Label 2 is older players 
#K means helps us find patterns 

#Comparing to implementation thats in sklearn 
'''from sklearn.cluster import KMeans 
kmeans = KMeans(3)
kmeans.fit(data)
KMeans(n_cluster=5)
centroids=kmeans.cluster_centers_
pd.DataFrame(centroids, columns=features).T'''
